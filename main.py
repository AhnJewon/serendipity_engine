import streamlit as st
import numpy as np
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
from SPARQLWrapper import SPARQLWrapper, JSON
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from kiwipiepy import Kiwi
import random
import json
from difflib import SequenceMatcher

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© (Streamlit ìºì‹±ìœ¼ë¡œ ì•± ì¬ì‹¤í–‰ ì‹œ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì§€ ì•Šë„ë¡ ìµœì í™”)
@st.cache_resource
def load_models():
    """ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë“¤ì„ ë¡œë“œ"""
    print("ì„ë² ë”© ëª¨ë¸ ë° í‚¤ì›Œë“œ ëª¨ë¸ ë¡œë”© ì¤‘...")
    embedding_model = SentenceTransformer('jhgan/ko-sbert-sts')
    kw_model = KeyBERT(embedding_model)
    kiwi = Kiwi()
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    return embedding_model, kw_model, kiwi

@st.cache_data
def load_concept_map(_embedding_model):
    """ìƒì„±ëœ ìœ„í‚¤ë°±ê³¼ ì˜ë¯¸ ì§€ë„ íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    title_path = 'korean_wiki_titles.json'
    vector_path = 'korean_wiki_vectors.npy'
    
    try:
        print("ëŒ€ê·œëª¨ ì˜ë¯¸ ì§€ë„ ë¡œë”© ì¤‘ (ì•½ê°„ì˜ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤)...")
        
        # 1. íƒ€ì´í‹€ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        with open(title_path, 'r', encoding='utf-8') as f:
            all_concepts = json.load(f)
            
        # 2. ë²¡í„° í–‰ë ¬ ë¡œë“œ
        all_concept_vectors = np.load(vector_path)
        
        print(f"ë¡œë”© ì™„ë£Œ! (ì´ {len(all_concepts)}ê°œì˜ ê°œë…)")
        return all_concepts, all_concept_vectors
        
    except FileNotFoundError:
        st.error("'ì˜ë¯¸ ì§€ë„' ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 'build_semantic_map.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë¹„ìƒìš© ë”ë¯¸ ë°ì´í„°
        dummy_concepts = ['ë°ì´í„° ì—†ìŒ', 'ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í•„ìš”']
        dummy_vectors = _embedding_model.encode(dummy_concepts)
        return dummy_concepts, dummy_vectors
    
embedding_model, kw_model, kiwi = load_models()
ALL_CONCEPTS, ALL_CONCEPT_VECTORS = load_concept_map(embedding_model)

# --- í•¨ìˆ˜ ì •ì˜ ---

def extract_noun_candidates(text):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ í›„ë³´ë“¤ì„ ì¶”ì¶œ"""
    tokens = kiwi.tokenize(text)
    candidates = []
    for t in tokens:
        # NNG(ì¼ë°˜ëª…ì‚¬), NNP(ê³ ìœ ëª…ì‚¬), SL(ì™¸êµ­ì–´)ë§Œ ì¶”ì¶œ
        if t.tag in ['NNG', 'NNP', 'SL']:
            candidates.append(t.form)
    
    # ì¤‘ë³µ ì œê±° ë° 2ê¸€ì ì´ìƒë§Œ ë‚¨ê¹€ (ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸)
    return list(set([c for c in candidates if len(c) >= 2]))

def extract_complex_candidates(text, valid_concepts_set):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ ë° ë³µí•© ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ë˜, 
    'ìœ„í‚¤ë°±ê³¼ í‘œì œì–´(valid_concepts_set)'ì— ì¡´ì¬í•˜ëŠ” ê°œë…ë§Œ ì¶”ì¶œì¶œ
    """
    print("\n[DEBUG]\textract_complex_candidates ì‹œì‘")
    print(f"[DEBUG]\tì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
    
    # 1. í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    lines = text.split('\n')
    candidates = set()
    all_extracted_nouns = []
    
    for line in lines:
        line = line.strip()
        if not line: continue
        
        tokens = kiwi.tokenize(line)
        
        nouns = []
        for t in tokens:
            # ëª…ì‚¬(NNG, NNP, NR, SL)ë§Œ ì¼ë‹¨ ìˆ˜ì§‘
            if t.tag in ['NNG', 'NNP', 'NR', 'SL']:
                nouns.append(t.form)
        
        if nouns:
            all_extracted_nouns.extend(nouns)
        
        if not nouns: continue
        
        n = len(nouns)
        
        # --- N-gram ìƒì„± ë° Whitelist ê²€ì¦ ---
        
        # 1. ë‹¨ì¼ ëª…ì‚¬ ê²€ì¦
        for noun in nouns:
            if len(noun) < 2: continue # 1ê¸€ìëŠ” ì œì™¸ (ì„ íƒì‚¬í•­)
            
            # ìœ„í‚¤ë°±ê³¼ í‘œì œì–´ì— ìˆëŠ” ê²½ìš°ì—ë§Œ í›„ë³´ë¡œ ë“±ë¡
            if noun in valid_concepts_set:
                candidates.add(noun)
                print(f"[DEBUG]\të‹¨ì¼ ëª…ì‚¬ ë§¤ì¹­: '{noun}'")
        
        # 2. ë³µí•© ëª…ì‚¬(2ë‹¨ì–´, 3ë‹¨ì–´) ê²€ì¦
        for i in range(n):
            # 2ë‹¨ì–´ ê²°í•©
            if i + 1 < n:
                bigram = f"{nouns[i]} {nouns[i+1]}" 
                bigram_nospace = f"{nouns[i]}{nouns[i+1]}"
                
                if (bigram in valid_concepts_set) or (bigram_nospace in valid_concepts_set):
                    candidates.add(bigram)
                    print(f"[DEBUG]\të³µí•©ëª…ì‚¬(2) ë§¤ì¹­: '{bigram}'")
            
            # 3ë‹¨ì–´ ê²°í•©
            if i + 2 < n:
                trigram = f"{nouns[i]} {nouns[i+1]} {nouns[i+2]}"
                trigram_nospace = f"{nouns[i]}{nouns[i+1]}{nouns[i+2]}"
                
                if (trigram in valid_concepts_set) or (trigram_nospace in valid_concepts_set):
                    candidates.add(trigram)
                    print(f"[DEBUG]\të³µí•©ëª…ì‚¬(3) ë§¤ì¹­: '{trigram}'")

    print(f"[DEBUG]\tì „ì²´ ì¶”ì¶œëœ ëª…ì‚¬: {set(all_extracted_nouns)}")
    print(f"[DEBUG]\tìµœì¢… í›„ë³´ ê°œìˆ˜: {len(candidates)}")
    print(f"[DEBUG]\tìµœì¢… í›„ë³´ ëª©ë¡: {list(candidates)}\n")
    return list(candidates)

def extract_key_concepts(search_history):
    """ìì—°ì–´ ê²€ìƒ‰ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ê°œë…ë“¤ì„ ì¶”ì¶œ"""
    if not search_history:
        return []
    
    # 1. ì›ë³¸ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    full_text = "\n".join(search_history)
    
    valid_concepts_set = set(ALL_CONCEPTS) 
    
    # 2. Kiwië¡œ ë³µí•© ëª…ì‚¬ í›„ë³´êµ° ì¶”ì¶œ
    complex_candidates = extract_complex_candidates(full_text, valid_concepts_set)
    print(f"ì¶”ì¶œëœ í›„ë³´êµ°: {complex_candidates}")
    
    if not complex_candidates:
        return []

    # ë³¸ë¬¸ ìì²´ë¥¼ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë„ì–´ì“°ê¸°
    tokenized_tokens = kiwi.tokenize(full_text)
    tokenized_text = " ".join([t.form for t in tokenized_tokens])
    print(f"\n[DEBUG]\tí˜•íƒœì†Œ ë¶„ë¦¬ í…ìŠ¤íŠ¸ (ì²˜ìŒ 200ì): {tokenized_text[:200]}...")

    # 3. ëª¨ë¸ ì‹¤í–‰
    print(f"[DEBUG]\tKeyBERT ì‹¤í–‰ - í›„ë³´êµ° ê°œìˆ˜: {len(complex_candidates)}")
    keywords_with_scores = kw_model.extract_keywords(
        tokenized_text, 
        candidates=complex_candidates, 
        keyphrase_ngram_range=(1, 3),
        top_n=5,
        use_mmr=True, 
        diversity=0.3
    )
    
    print(f"\n[DEBUG]\tKeyBERT ê²°ê³¼ (í‚¤ì›Œë“œ, ì ìˆ˜):")
    for kw, score in keywords_with_scores:
        print(f"  - {kw}: {score:.4f}")
    print()

    return [keyword for keyword, score in keywords_with_scores]

def get_interest_nebula_vector(concepts, embedding_model):
    """ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì˜ ë²¡í„°ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ 'ê´€ì‹¬ ì„±ìš´' ë²¡í„° ìƒì„±"""
    if not concepts:
        return None
    
    print(f"\n[DEBUG]\tê´€ì‹¬ ì„±ìš´ ë²¡í„° ìƒì„±")
    print(f"[DEBUG]\tì…ë ¥ ê°œë…: {concepts}")
    
    concept_vectors = embedding_model.encode(concepts)
    # ìµœì‹ (ë˜ëŠ” ìƒìœ„) í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
    weights = np.linspace(0.8, 1.2, len(concepts))
    print(f"[DEBUG]\tê°€ì¤‘ì¹˜: {weights}")
    
    weighted_avg_vector = np.average(concept_vectors, axis=0, weights=weights)
    print(f"[DEBUG]\tìƒì„±ëœ ë²¡í„° shape: {weighted_avg_vector.shape}\n")
    
    return weighted_avg_vector

def find_semantic_antipode_by_clustering(interest_vector, n_clusters=5, pool_size=1000):
    """
    1. ê´€ì‹¬ì‚¬ì™€ ê°€ì¥ ê±°ë¦¬ê°€ ë¨¼ pool_sizeê°œì˜ ê°œë…ì„ 1ì°¨ ì„ ë³„
    2. ì„ ë³„ëœ ê°œë…ë“¤ì„ K-Meansë¡œ n_clustersê°œë¡œ êµ°ì§‘í™”
    3. ê° êµ°ì§‘ì˜ ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´ë¥¼ ëŒ€í‘œ í‚¤ì›Œë“œë¡œ ë°˜í™˜
    """
    if interest_vector is None:
        return []

    # 1. ì „ì²´ ê°œë…ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    print(f"\n[DEBUG]\të°˜ëŒ€í¸ íƒìƒ‰ ì‹œì‘ (pool_size={pool_size}, n_clusters={n_clusters})")
    user_sims = cosine_similarity(interest_vector.reshape(1, -1), ALL_CONCEPT_VECTORS)[0]
    print(f"[DEBUG]\tìœ ì‚¬ë„ ë²”ìœ„: {user_sims.min():.4f} ~ {user_sims.max():.4f}")

    # 2. ìœ ì‚¬ë„ê°€ ê°€ì¥ ë‚®ì€ í•˜ìœ„ pool_sizeê°œ ì¸ë±ìŠ¤ ì¶”ì¶œ (ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬)
    far_indices = np.argsort(user_sims)[:pool_size]
    print(f"[DEBUG]\tê°€ì¥ ë¨¼ {pool_size}ê°œ ê°œë… ì„ ë³„ ì™„ë£Œ")
    print(f"[DEBUG]\tìµœí•˜ìœ„ ìœ ì‚¬ë„ ìƒ˜í”Œ (5ê°œ): {user_sims[far_indices[:5]]}")
    
    far_vectors = ALL_CONCEPT_VECTORS[far_indices]
    far_concepts = [ALL_CONCEPTS[i] for i in far_indices]
    print(f"[DEBUG]\tìƒ˜í”Œ ê°œë…ë“¤: {far_concepts[:10]}")
    
    # 3. K-Means êµ°ì§‘í™” ìˆ˜í–‰
    print(f"[DEBUG]\tK-Means êµ°ì§‘í™” ì‹œì‘...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=None, n_init='auto')
    kmeans.fit(far_vectors)
    print(f"[DEBUG]\têµ°ì§‘í™” ì™„ë£Œ (inertia: {kmeans.inertia_:.2f})")
    
    centers = kmeans.cluster_centers_
    cluster_labels = kmeans.labels_
    
    representative_concepts = []
    
    # 4. ê° êµ°ì§‘ë³„ ëŒ€í‘œ ë‹¨ì–´ ì¶”ì¶œ
    print(f"\n[DEBUG]\tê° êµ°ì§‘ì˜ ëŒ€í‘œ ë‹¨ì–´ ì„ ì •:")
    for i in range(n_clusters):
        # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ë²¡í„°ë“¤ ë§ˆìŠ¤í‚¹
        mask = (cluster_labels == i)
        if not np.any(mask):
            print(f"  [êµ°ì§‘ {i}] ë¹„ì–´ìˆìŒ")
            continue
            
        cluster_vectors = far_vectors[mask]
        cluster_concepts_list = np.array(far_concepts)[mask]
        print(f"  [êµ°ì§‘ {i}] í¬ê¸°: {len(cluster_concepts_list)}ê°œ")
        
        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        sims_to_center = cosine_similarity(centers[i].reshape(1, -1), cluster_vectors)[0]
        
        # ì¤‘ì‹¬ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´ ì„ íƒ
        best_idx = np.argmax(sims_to_center)
        representative = cluster_concepts_list[best_idx]
        representative_concepts.append(representative)
        print(f"  [êµ°ì§‘ {i}] ëŒ€í‘œ: '{representative}' (ì¤‘ì‹¬ ìœ ì‚¬ë„: {sims_to_center[best_idx]:.4f})")
        print(f"  [êµ°ì§‘ {i}] ìƒ˜í”Œ: {list(cluster_concepts_list[:5])}")
    
    print(f"\n[DEBUG]\tìµœì¢… ëŒ€í‘œ ê°œë…ë“¤: {representative_concepts}\n")
    return representative_concepts

def check_and_promote_concept(concept):
    """
    ê°œë… ìŠ¹ê²© ì‹œ, 'ëª©ë¡', 'ë™ìŒì´ì˜ì–´' ê°™ì€ ë©”íƒ€ ë°ì´í„°ë¥¼ ì œì™¸
    """
    print(f"\n[DEBUG]\tê°œë… ìŠ¹ê²©(ì¶”ìƒí™”) ì‹œë„: '{concept}'")
    endpoint_url = "https://query.wikidata.org/sparql"
    sparql = SPARQLWrapper(endpoint_url)
    
    query = f"""
    SELECT ?parentLabel WHERE {{
      SERVICE wikibase:mwapi {{
        bd:serviceParam wikibase:api "EntitySearch".
        bd:serviceParam wikibase:endpoint "www.wikidata.org".
        bd:serviceParam mwapi:search "{concept}".
        bd:serviceParam mwapi:language "ko".
        ?item wikibase:apiOutputItem mwapi:item.
      }}
      
      ?item wdt:P31|wdt:P279|wdt:P136 ?parent.
      
      # ë¬´ì˜ë¯¸í•œ ë©”íƒ€ ë°ì´í„° ID ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
      # Q13406463: ìœ„í‚¤ë¯¸ë””ì–´ ëª©ë¡ (Wikimedia list article)
      # Q4167410: ë™ìŒì´ì˜ì–´ ë¬¸ì„œ (Wikipedia disambiguation page)
      # Q11266439: í…œí”Œë¦¿ (Template)
      FILTER(?parent NOT IN (wd:Q13406463, wd:Q4167410, wd:Q11266439))

      SERVICE wikibase:label {{ 
        bd:serviceParam wikibase:language "ko". 
        ?parent rdfs:label ?parentLabel. 
      }}
    }} LIMIT 1
    """
    
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    sparql.setTimeout(3) 
    
    try:
        results = sparql.query().convert()
        bindings = results["results"]["bindings"]
        
        if bindings:
            promoted = bindings[0].get("parentLabel", {}).get("value")
            # ì˜ì–´ ì´ë¦„ì´ ê·¸ëŒ€ë¡œ ë‚˜ì˜¤ê±°ë‚˜ ì…ë ¥ê³¼ ê°™ìœ¼ë©´ íŒ¨ìŠ¤
            if promoted and promoted != concept: 
                print(f"[DEBUG]\tìŠ¹ê²© ì„±ê³µ: '{concept}' -> '{promoted}'")
                return promoted, True
                
        print(f"[DEBUG]\t- ìŠ¹ê²© ì‹¤íŒ¨ (ìœ íš¨í•œ ìƒìœ„ ê°œë… ì—†ìŒ)")
        return concept, False
        
    except Exception as e:
        print(f"[DEBUG]\t! SPARQL ì—ëŸ¬: {e}")
        return concept, False
        
    except Exception as e:
        print(f"[DEBUG]\t! SPARQL ì—ëŸ¬: {e}")
        return concept, False

def is_string_too_similar(s1, s2):
    """ë‘ ë‹¨ì–´ì˜ ê¸€ì êµ¬ì„±ì´ ë„ˆë¬´ ë¹„ìŠ·í•˜ë©´ True ë°˜í™˜"""
    s1_clean = s1.replace(" ", "")
    s2_clean = s2.replace(" ", "")
    
    # 1. í¬í•¨ ê´€ê³„
    if s1_clean in s2_clean or s2_clean in s1_clean:
        return True
        
    # 2. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
    # 40% ì´ìƒ ê¸€ìê°€ ê²¹ì¹˜ë©´ ì°¨ë‹¨
    if SequenceMatcher(None, s1_clean, s2_clean).ratio() > 0.4:
        return True
        
    return False

# ì „ì—­ ë³€ìˆ˜ë¥¼ ì§ì ‘ ì°¸ì¡°í•˜ì—¬ ì—°ê²°ê³ ë¦¬ ì°¾ê¸°
def find_multi_step_bridge(start_concept, end_concept):
    """
    ALL_CONCEPTS, ALL_CONCEPT_VECTORS, embedding_model ì „ì—­ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print(f"\n[DEBUG]\të²¡í„° ì—°ê²°ê³ ë¦¬ ìƒì„±: '{start_concept}' -> ... -> '{end_concept}'")
    
    # 1. ì‹œì‘ì  ë²¡í„° ì°¾ê¸°
    try:
        idx1 = ALL_CONCEPTS.index(start_concept)
        v1 = ALL_CONCEPT_VECTORS[idx1]
    except ValueError:
        # ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì „ì—­ ëª¨ë¸ë¡œ ì¦‰ì‹œ ì¸ì½”ë”©
        v1 = embedding_model.encode([start_concept])[0]

    # 2. ëì  ë²¡í„° ì°¾ê¸°
    try:
        idx2 = ALL_CONCEPTS.index(end_concept)
        v2 = ALL_CONCEPT_VECTORS[idx2]
    except ValueError:
        v2 = embedding_model.encode([end_concept])[0]

    path = []
    # ì¤‘ë³µ ë°©ì§€ ì§‘í•©
    seen_concepts = {start_concept, end_concept}
    
    # 3. 3ë‹¨ê³„ ë³´ê°„ (25%, 50%, 75%)
    steps = [0.25, 0.50, 0.75]
    
    for t in steps:
        interpolated_vector = (1 - t) * v1 + t * v2
        
        # ì „ì—­ ë²¡í„° í–‰ë ¬ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
        sims = cosine_similarity(interpolated_vector.reshape(1, -1), ALL_CONCEPT_VECTORS)[0]
        
        top_indices = np.argsort(sims)[::-1][:100]
        
        found_step = None
        for idx in top_indices:
            candidate = ALL_CONCEPTS[idx]
            
            # [í•„í„°ë§ 1] ì´ë¯¸ ë‚˜ì˜¨ ë‹¨ì–´ ì œì™¸
            if candidate in seen_concepts:
                continue
            
            # [í•„í„°ë§ 2] ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸
            if len(candidate) < 2:
                continue

            # [í•„í„°ë§ 3] ê¸€ì ì¤‘ë³µ/ìœ ì‚¬ë„ ì²´í¬
            if is_string_too_similar(start_concept, candidate):
                continue
            if is_string_too_similar(end_concept, candidate):
                continue
            
            # í†µê³¼
            found_step = candidate
            seen_concepts.add(candidate)
            break
        
        if found_step:
            path.append(found_step)
            print(f"[DEBUG]\t{int(t*100)}% ì§€ì  ë°œê²¬: '{found_step}'")
        else:
            path.append("ì—°ê´€ ê°œë…")

    return path

def get_vector(word):
    """ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        idx = ALL_CONCEPTS.index(word)
        return ALL_CONCEPT_VECTORS[idx]
    except ValueError:
        return embedding_model.encode([word])[0]

def fill_gap_between(w1, w2, exclude_list=None):
    if exclude_list is None: exclude_list = set()
    
    v1 = get_vector(w1)
    v2 = get_vector(w2)
    mid_vector = (v1 + v2) / 2
    
    sims = cosine_similarity(mid_vector.reshape(1, -1), ALL_CONCEPT_VECTORS)[0]
    top_indices = np.argsort(sims)[::-1][:30]
    
    for idx in top_indices:
        candidate = ALL_CONCEPTS[idx]
        
        # ì œì™¸ ëª©ë¡ì— ìˆê±°ë‚˜, ê¸€ìê°€ ë„ˆë¬´ ë¹„ìŠ·í•˜ë©´ íŒ¨ìŠ¤
        if candidate in exclude_list: continue
        if candidate in [w1, w2]: continue
        if is_string_too_similar(w1, candidate): continue
        if is_string_too_similar(w2, candidate): continue
            
        return candidate
            
    return None

def smooth_path_recursively(full_path, threshold=0.5):
    print(f"[DEBUG]\tê²½ë¡œ í‰íƒ„í™” ì‹œì‘: {full_path}")
    refined_path = [full_path[0]] 
    
    for i in range(len(full_path) - 1):
        curr_word = full_path[i]
        next_word = full_path[i+1]
        
        v1 = get_vector(curr_word)
        v2 = get_vector(next_word)
        
        similarity = cosine_similarity(v1.reshape(1, -1), v2.reshape(1, -1))[0][0]
        
        # Gap ë°œê²¬ ì‹œ ë³´ìˆ˜ ê³µì‚¬
        if similarity < threshold:
            print(f"[DEBUG]\tGap ë°œê²¬ ({curr_word} <-> {next_word}, sim={similarity:.2f})")
            
            # ì´ë¯¸ ë‚˜ì˜¨ ë‹¨ì–´ë“¤ì„ í”¼í•˜ë„ë¡ í•¨
            # í˜„ì¬ê¹Œì§€ í™•ì •ëœ ê²½ë¡œ(refined_path)ì™€ ë‹¤ìŒ ëª©ì ì§€(next_word)ë¥¼ ë°°ì œ ëª©ë¡ìœ¼ë¡œ ì „ë‹¬
            exclude_words = set(refined_path + [next_word])
            
            gap_filler = fill_gap_between(curr_word, next_word, exclude_words)
            
            if gap_filler:
                print(f"[DEBUG]\të³´ê°• ì™„ë£Œ: {gap_filler}")
                refined_path.append(gap_filler)
            else:
                print(f"[DEBUG]\të³´ê°• ì‹¤íŒ¨")
        
        refined_path.append(next_word)
        
    return refined_path

# --- 3. Streamlit UI êµ¬ì„± ---

st.set_page_config(layout="wide", page_title="Serendipity Engine")

st.title("ì„¸ë Œë””í”¼í‹° ê²€ìƒ‰ ì—”ì§„")
st.markdown("""
> *"ìš°ë¦¬ëŠ” ìš°ë¦¬ê°€ ë¬´ì—‡ì„ ëª¨ë¥´ëŠ”ì§€ ëª¨ë¦…ë‹ˆë‹¤."* ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ ë„ˆë¨¸, ì™„ì „íˆ ìƒˆë¡œìš´ ì˜ê°ì˜ ì„¸ê³„ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.
""")

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ğŸ” ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ ì…ë ¥")
    search_history_input = st.text_area(
        "ìµœê·¼ ê´€ì‹¬ìˆê²Œ ì°¾ì•„ë³¸ ì£¼ì œë‚˜ ê³ ë¯¼ë“¤ì„ ììœ ë¡­ê²Œ ì ì–´ì£¼ì„¸ìš”.", 
        height=200,
        placeholder="ì˜ˆì‹œ:\n- ìµœê·¼ ìƒì„±í˜• AIì˜ ì €ì‘ê¶Œ ë…¼ë€ì— ëŒ€í•´ ì°¾ì•„ë´„\n- ë§›ìˆëŠ” ì»¤í”¼ë¥¼ ë‚´ë¦¬ëŠ” ë¸Œë£¨ì‰ ë ˆì‹œí”¼\n- ì¡°ì„ ì™•ì¡°ì‹¤ë¡ ì¤‘ ì„¸ì¢…ëŒ€ì™•ì˜ ì—…ì "
    )
    start_btn = st.button("ìƒˆë¡œìš´ ë°œê²¬ ì‹œì‘í•˜ê¸°", use_container_width=True)

with col2:
    st.subheader("ğŸ’¡ ë°œê²¬ ê²°ê³¼")
    result_container = st.container()

if start_btn:
    if not search_history_input.strip():
        st.warning("ê´€ì‹¬ì‚¬ë¥¼ ì…ë ¥í•´ì•¼ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        history_list = [line.strip() for line in search_history_input.split('\n') if line.strip()]
        
        with result_container:
            # 1. ê´€ì‹¬ ì„±ìš´ ë¶„ì„
            with st.status("1. ì§€ì  ì„±ìš´(Interest Nebula) ë¶„ì„ ì¤‘...", expanded=True) as status:
                concepts = extract_key_concepts(history_list)
                
                if not concepts:
                    status.update(label="í•µì‹¬ ê°œë… ì¶”ì¶œ ì‹¤íŒ¨! ë¬¸ì¥ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ì£¼ì„¸ìš”.", state="error")
                else:
                    st.write(f"**ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ:** {', '.join(concepts)}")
                    interest_vector = get_interest_nebula_vector(concepts, embedding_model)
                    
                    # 2. ë™ì  êµ°ì§‘í™”ë¥¼ í†µí•œ ë°˜ëŒ€í¸ íƒìƒ‰
                    status.update(label="2. ë¯¸ì§€ì˜ ì˜ì—­(Antipode) êµ°ì§‘í™” ë° íƒì‚¬ ì¤‘...", state="running")
                    
                    # 5ê°œì˜ ë‚¯ì„  í…Œë§ˆë¥¼ ë„ì¶œ
                    antipode_themes = find_semantic_antipode_by_clustering(interest_vector, n_clusters=5)
                    st.write(f"**ë°œê²¬ëœ ë‚¯ì„  í…Œë§ˆë“¤:** {', '.join(antipode_themes)}")
                    
                    # 3. ì—°ê²° ê³ ë¦¬ ê±´ì„¤
                    status.update(label="3. ë…¼ë¦¬ì  ì—°ê²° ê³ ë¦¬(Bridge) ê±´ì„¤ ì¤‘...", state="running")
                    
                    main_concept = concepts[0] # ê°€ì¥ ë¹„ì¤‘ ìˆëŠ” ë‚´ ê´€ì‹¬ì‚¬
                    final_path = None
                    
                    progress_text = st.empty()
                    prog_bar = st.progress(0)
                    
                    for i, candidate in enumerate(antipode_themes):
                        prog_bar.progress((i + 1) / len(antipode_themes))
                        
                        # 1. ê°œë… ìŠ¹ê²©
                        promoted_cand, is_promoted = check_and_promote_concept(candidate)
                        
                        # 2. ë¸Œë¦¿ì§€ íƒìƒ‰ (ì¶œë°œì§€ <-> ìŠ¹ê²©ëœ ìƒìœ„ ê°œë…)
                        initial_bridges = find_multi_step_bridge(main_concept, promoted_cand)
                        
                        # 3. ê²½ë¡œê°€ ìœ íš¨í•˜ë©´ ì±„íƒ
                        if len(initial_bridges) == 3:
                            # 2. ì „ì²´ ê²½ë¡œ ì¡°ë¦½
                            raw_path = [main_concept] + initial_bridges + [promoted_cand]

                            # 3. Gap Filling ìˆ˜í–‰
                            # threshold=0.5: ìœ ì‚¬ë„ê°€ 0.5 ë¯¸ë§Œì´ë©´ ì¤‘ê°„ì— í•˜ë‚˜ ë” ë¼ì›Œë„£ìŒ
                            smoothed_path_list = smooth_path_recursively(raw_path, threshold=0.5)

                            # 4. ê²°ê³¼ ì €ì¥
                            final_path = {
                                "full_chain": smoothed_path_list, # ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
                                "start": main_concept,
                                "end": candidate,
                                "context": promoted_cand
                            }
                            break
                    
                    prog_bar.empty()
                    status.update(label="íƒì‚¬ ì™„ë£Œ!", state="complete", expanded=False)

            # --- ìµœì¢… ê²°ê³¼ ì¹´ë“œ í‘œì‹œ (UI ê°œì„ ) ---
            if final_path:
                st.balloons()
                st.success(f"###ìƒˆë¡œìš´ íƒí—˜ì§€ ë°œê²¬: [{final_path['end']}]")
                st.markdown("---")

                # ì „ì²´ ê²½ë¡œ ì‹œê°í™”
                display_chain = final_path['full_chain'] + [f"**{final_path['end']}**"]

                path_str = " â” ".join(display_chain)

                st.markdown("### ğŸ”— ì—°ê²° ê²½ë¡œ")
                st.info(path_str)

                st.markdown("---")
                mid_concepts = final_path['full_chain'][1:-1] # ì¤‘ê°„ ë‹¨ê³„ë“¤ë§Œ ì¶”ì¶œ
                mid_concepts_str = ", ".join([f"**[{c}]**" for c in mid_concepts])
