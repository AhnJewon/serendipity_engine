import streamlit as st
import numpy as np
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
from SPARQLWrapper import SPARQLWrapper, JSON
from sklearn.metrics.pairwise import cosine_similarity
import random
import json

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© (Streamlit ìºì‹±ìœ¼ë¡œ ì•± ì¬ì‹¤í–‰ ì‹œ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì§€ ì•Šë„ë¡ ìµœì í™”)
@st.cache_resource
def load_models():
    """ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ì„ë² ë”© ëª¨ë¸ ë° í‚¤ì›Œë“œ ëª¨ë¸ ë¡œë”© ì¤‘...")
    embedding_model = SentenceTransformer('jhgan/ko-sbert-sts')
    kw_model = KeyBERT(embedding_model)
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    return embedding_model, kw_model

@st.cache_data
def load_concept_map(_embedding_model):
    """ìƒì„±ëœ ìœ„í‚¤ë°±ê³¼ ì˜ë¯¸ ì§€ë„ íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    title_path = 'korean_wiki_titles.json'
    vector_path = 'korean_wiki_vectors.npy'
    
    try:
        print("ëŒ€ê·œëª¨ ì˜ë¯¸ ì§€ë„ ë¡œë”© ì¤‘ (ì•½ê°„ì˜ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤)...")
        
        # 1. íƒ€ì´í‹€ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        with open(title_path, 'r', encoding='utf-8') as f:
            all_concepts = json.load(f)
            
        # 2. ë²¡í„° í–‰ë ¬ ë¡œë“œ
        all_concept_vectors = np.load(vector_path)
        
        print(f"ë¡œë”© ì™„ë£Œ! (ì´ {len(all_concepts)}ê°œì˜ ê°œë…)")
        return all_concepts, all_concept_vectors
        
    except FileNotFoundError:
        st.error("'ì˜ë¯¸ ì§€ë„' ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 'build_semantic_map.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë¹„ìƒìš© ë”ë¯¸ ë°ì´í„°
        dummy_concepts = ['ë°ì´í„° ì—†ìŒ', 'ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í•„ìš”']
        dummy_vectors = _embedding_model.encode(dummy_concepts)
        return dummy_concepts, dummy_vectors
    
embedding_model, kw_model = load_models()
ALL_CONCEPTS, ALL_CONCEPT_VECTORS = load_concept_map(embedding_model)

# --- í•¨ìˆ˜ ì •ì˜ ---

def extract_key_concepts(search_history):
    """ìì—°ì–´ ê²€ìƒ‰ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ê°œë…ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not search_history:
        return []
    
    full_text = " ".join(search_history)
    
    keywords_with_scores = kw_model.extract_keywords(
        full_text, 
        keyphrase_ngram_range=(1, 2), 
        stop_words=None, 
        top_n=10
    )
    
    # (í‚¤ì›Œë“œ, ì ìˆ˜) íŠœí”Œì—ì„œ í‚¤ì›Œë“œ(ë¬¸ìì—´)ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
    concepts = [keyword for keyword, score in keywords_with_scores]
    return concepts

def get_interest_nebula_vector(concepts, embedding_model):
    """í•µì‹¬ ê°œë… ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ 'ê´€ì‹¬ ì„±ìš´' ì¤‘ì‹¬ ë²¡í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if not concepts:
        return None
    
    concept_vectors = embedding_model.encode(concepts)
    weights = np.linspace(0.5, 1.5, len(concepts)) # ìµœì‹  ê²€ìƒ‰ì–´ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    weighted_avg_vector = np.average(concept_vectors, axis=0, weights=weights)
    
    return weighted_avg_vector

def find_semantic_antipode(interest_vector, concept_vectors, all_concepts, top_n=30, diversity=0.4):
    """
    ê´€ì‹¬ ë²¡í„°ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ë©€ë©´ì„œ, ë™ì‹œì— ì„œë¡œ ë‹¤ì–‘í•œ ê°œë…ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    if interest_vector is None:
        return []

    # 1. ëª¨ë“  ê°œë…ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    user_sims = cosine_similarity(interest_vector.reshape(1, -1), concept_vectors)[0]
   
    # 2. ì´ˆê¸° í›„ë³´êµ° ì„ ì •
    pool_size = 500
    sorted_indices = np.argsort(user_sims) # ì˜¤ë¦„ì°¨ìˆœ(ìœ ì‚¬ë„ ë‚®ì€ ìˆœ)
    candidate_indices = sorted_indices[:pool_size]
    
    selected_indices = []
    
    # 3. Greedy Selection (MMR ì•Œê³ ë¦¬ì¦˜)
    for _ in range(top_n):
        best_idx = -1
        best_score = -float('inf')

        for idx in candidate_indices:
            if idx in selected_indices:
                continue
            
            # A. ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ì˜ ê±°ë¦¬ (ë©€ìˆ˜ë¡ ì¢‹ìŒ)
            dist_to_user = 1 - user_sims[idx]
            
            # B. ì´ë¯¸ ì„ íƒëœ ê°œë…ë“¤ê³¼ì˜ ê±°ë¦¬ (ë‹¤ì–‘ì„±)
            if not selected_indices:
                dist_to_selected = 1.0 
            else:
                selected_vectors = concept_vectors[selected_indices]
                current_vector = concept_vectors[idx].reshape(1, -1)
                sims_to_selected = cosine_similarity(current_vector, selected_vectors)[0]
                dist_to_selected = 1 - np.max(sims_to_selected)
            
            # ì ìˆ˜ ê³„ì‚° (ë‹¤ì–‘ì„± ë°˜ì˜)
            score = (1 - diversity) * dist_to_user + (diversity * dist_to_selected)
            
            if score > best_score:
                best_score = score
                best_idx = idx
        
        if best_idx != -1:
            selected_indices.append(best_idx)
            
    return [all_concepts[i] for i in selected_indices]


def find_bridge_keywords(concept1, concept2):
    """Wikidataì—ì„œ ë‘ ê°œë… ì‚¬ì´ì˜ ì—°ê²° ê²½ë¡œë¥¼ íƒìƒ‰í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    endpoint_url = "https://query.wikidata.org/sparql"
    
    # ë‘ ê°œë…ì„ ì‡ëŠ” ì¤‘ê°„ ê°œë…(ë“¤)ì„ ì°¾ëŠ” ì˜ˆì‹œ ì¿¼ë¦¬
    # P31(instance of) ë˜ëŠ” P279(subclass of) ì†ì„±ì„ ë”°ë¼ ìµœëŒ€ 5ë‹¨ê³„ê¹Œì§€ íƒìƒ‰
    query = f"""
    SELECT ?bridgeLabel WHERE {{
      SERVICE wikibase:mwapi {{
        bd:serviceParam wikibase:api "EntitySearch".
        bd:serviceParam wikibase:endpoint "www.wikidata.org".
        bd:serviceParam mwapi:search "{concept1}".
        bd:serviceParam mwapi:language "ko".
        ?concept1 wikibase:apiOutputItem mwapi:item.
      }}
      SERVICE wikibase:mwapi {{
        bd:serviceParam wikibase:api "EntitySearch".
        bd:serviceParam wikibase:endpoint "www.wikidata.org".
        bd:serviceParam mwapi:search "{concept2}".
        bd:serviceParam mwapi:language "ko".
        ?concept2 wikibase:apiOutputItem mwapi:item.
      }}
      
      ?concept1 (wdt:P31|wdt:P279)* ?bridge.
      ?concept2 (wdt:P31|wdt:P279)* ?bridge.
      
      FILTER(?concept1 != ?concept2 && ?bridge != wd:Q35120) # Entity(ìµœìƒìœ„ í´ë˜ìŠ¤)ëŠ” ì œì™¸
      
      SERVICE wikibase:label {{ bd:serviceParam wikibase:language "ko". }}
    }} LIMIT 5
    """
    
    sparql = SPARQLWrapper(endpoint_url)
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    sparql.setTimeout(10)
            
    try:
        results = sparql.query().convert()
        keywords = [result["bridgeLabel"]["value"] for result in results["results"]["bindings"]]
        return list(set(keywords)) # ì¤‘ë³µ ì œê±°
    except Exception as e:
        print(f"SPARQL ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
        return []

# Streamlit UI êµ¬ì„± 

st.set_page_config(layout="wide")
st.title("ì„¸ë Œë””í”¼í‹° ê²€ìƒ‰ ì—”ì§„")
st.write("ë‹¹ì‹ ì˜ ìƒê°ì˜ ê°ì˜¥ì„ ê¹¨ëœ¨ë¦´ ì˜ì™¸ì˜ ì—°ê²°ê³ ë¦¬ë¥¼ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.")

# ì‚¬ìš©ì ì…ë ¥
search_history_input = st.text_area(
    "ìµœê·¼ ê´€ì‹¬ìˆê²Œ ì°¾ì•„ë³¸ ì£¼ì œë‚˜ ì§ˆë¬¸ë“¤ì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•´ì£¼ì„¸ìš”.", 
    height=150,
    placeholder="ì˜ˆì‹œ:\nìµœê·¼ AI ìœ¤ë¦¬ ë¬¸ì œê°€ ì‹¬ê°í•œë° ìŠ¤í† ì•„ ì² í•™ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œ?\nììœ¨ì£¼í–‰ì°¨ì˜ íŠ¸ë¡¤ë¦¬ ë”œë ˆë§ˆ ì‚¬ë¡€ ë¶„ì„"
)

if st.button("ìƒˆë¡œìš´ ë°œê²¬ ì‹œì‘í•˜ê¸°"):
    if search_history_input:
        history_list = [line.strip() for line in search_history_input.split('\n') if line.strip()]
        
        if len(history_list) < 2:
            st.warning("ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ 2ê°œ ì´ìƒì˜ ê´€ì‹¬ì‚¬ë¥¼ ì…ë ¥í•´ì£¼ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤.")

        with st.spinner("1. ë‹¹ì‹ ì˜ ì§€ì  ì„±ìš´(Interest Nebula)ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            # Phase 1: í•µì‹¬ ê°œë… ì¶”ì¶œ ë° ë²¡í„°í™”
            concepts = extract_key_concepts(history_list)
            
            if not concepts:
                st.error("ì…ë ¥ì—ì„œ ìœ ì˜ë¯¸í•œ í•µì‹¬ ê°œë…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” ìì„¸íˆ ì ì–´ì£¼ì„¸ìš”.")
            else:
                st.info(f"**ğŸ” ë¶„ì„ëœ í•µì‹¬ í‚¤ì›Œë“œ:** {', '.join(concepts)}")
                interest_vector = get_interest_nebula_vector(concepts, embedding_model)
                
                # Phase 2: ì˜ë¯¸ì  ë°˜ëŒ€í¸ íƒìƒ‰ (MMR ì ìš©)
                with st.spinner("2. ì˜ë¯¸ì˜ ìš°ì£¼ë¥¼ íƒìƒ‰í•˜ì—¬ ë‚¯ì„  í–‰ì„±(Antipode)ì„ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    # ë„‰ë„‰í•˜ê²Œ 30ê°œë¥¼ ë½‘ìŠµë‹ˆë‹¤ (í•„í„°ë§ ë° ì—°ê²°ì„± ê²€ì¦ì„ ìœ„í•´)
                    candidates = find_semantic_antipode(interest_vector, ALL_CONCEPT_VECTORS, ALL_CONCEPTS, top_n=30, diversity=0.4)
                
                # Phase 3: í•„í„°ë§ ë° ì—°ê²° ê³ ë¦¬ ê²€ì¦ (ê¸°ì¡´ì˜ ë‹¨ìˆœ random.choice ëŒ€ì‹  ê²€ì¦ ë£¨í”„ ì‚¬ìš©)
                with st.spinner("3. ë…¼ë¦¬ì  ì—°ê²° ê³ ë¦¬(Bridge)ë¥¼ ê±´ì„¤ ì¤‘ì…ë‹ˆë‹¤..."):
                    main_concept = concepts[0] # ê°€ì¥ ë¹„ì¤‘ ìˆëŠ” í‚¤ì›Œë“œ
                    final_antipode = None
                    final_bridges = []
                    
                    progress_bar = st.progress(0)
                    
                    for i, candidate in enumerate(candidates):
                        progress_bar.progress((i + 1) / len(candidates))
                        
                        # ì—°ê²° ê³ ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (SPARQL)
                        bridges = find_bridge_keywords(main_concept, candidate)
                        if bridges:
                            final_antipode = candidate
                            final_bridges = bridges
                            break
                    
                    progress_bar.empty() # ì§„í–‰ë°” ìˆ¨ê¸°ê¸°

                # ìµœì¢… ê²°ê³¼ ì¶œë ¥
                if final_antipode:
                    st.success(f"ğŸ¯ **ìƒˆë¡œìš´ íƒí—˜ ì˜ì—­ ë°œê²¬:** #{final_antipode}")
                    st.markdown("---")
                    
                    # ì—°ê²° ê³ ë¦¬ ì‹œê°í™”
                    path_steps = [f"**{main_concept}**"] + [f"`{b}`" for b in final_bridges] + [f"**{final_antipode}**"]
                    path_md = " â¡ï¸ ".join(path_steps)
                    
                    st.write("ë‹¤ìŒì˜ ë…¼ë¦¬ì  ê²½ë¡œë¥¼ í†µí•´ ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ì™€ ì—°ê²°ë©ë‹ˆë‹¤:")
                    st.info(path_md)
                    
                    st.caption(f"ğŸ’¡ '{main_concept}'ì™€(ê³¼) '{final_antipode}' ì‚¬ì´ì˜ ê´€ê³„ë¥¼ Wikidata ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ì•„ì‰½ê²Œë„ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²° ê°€ëŠ¥í•œ 'ì˜ë¯¸ì  ë°˜ëŒ€í¸'ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    st.write("ê´€ì‹¬ì‚¬ì™€ ë„ˆë¬´ ë™ë–¨ì–´ì§„ ê°œë…ë§Œ ë‚¨ì•˜ê±°ë‚˜, ì§€ì‹ ê·¸ë˜í”„ ì—°ê²°ì´ ëŠê²¨ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì£¼ì œë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”!")
                    if candidates:
                        st.write(f"(ì°¸ê³ : í›„ë³´ë¡œ '{candidates[0]}' ë“±ì´ ë°œê²¬ë˜ì—ˆìœ¼ë‚˜ ì—°ê²° ê³ ë¦¬ê°€ ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤.)")
                        print(f"Debug: Candidates were {candidates}")

    else:
        st.error("ê²€ìƒ‰ ê¸°ë¡ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
